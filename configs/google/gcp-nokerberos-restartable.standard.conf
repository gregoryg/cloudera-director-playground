# Cluster name
environmentName: "c-137"
deploymentName: gazorpazorpfield #teenyverse
name: enchiladas #zeep

#
# Cloud provider configuration
provider {
  type: google
  #
  # The Project ID
  # See: https://cloud.google.com/storage/docs/projects#projectid
  #
  projectId: gcp-se
  #
  # The JSON key for the service account that Director should use
  # See: https://cloud.google.com/storage/docs/authentication#service_accounts
  #
  jsonKey: """
{
  "type": "service_account",
  "project_id": "gcp-se",
  "private_key_id": "7fa4b95be656ac8faef6c54b00031292c800f724",
  "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCh8WvjVY7zYoEn\nqacbu2JKsuUBHBE7RkzADEIZG11hUwDNxOB6VvMvarO941wWVRNgYPHMtIfdu4TH\nGhn6l+3o6B9ddlSbb7oWW+RA7OG6VqvYpWI9A4/7htg42Pwx56avrSE7BOC6x2+n\nLA5IL7KIVWMZEeXzQ6C+G77cERK1dMIqgEtG/Joe1cGhSVb11+SP9KeinUkoLjYc\nbg/Za90jeyPUOWkKMc5JHbIsWQrjP+dLZMpA8EcSpcBGC/jdE0AuhHUotMj+rwcV\nWn+uC3OuEKiYeBv8qYmKa3jprKR/TSSCFAsE3NL4L2Blmjni6+vM8kussg7lOtLQ\ni80tHtbjAgMBAAECggEASIEsiqXd0L5DkRx2AwaMZWFfAnYsiNtatYlHdrI3xsFA\n1N/WmS+gxZQCUXwf2KfSew4sdDhHtFCmi/X1t6qaC/MRR+kYzFbje+s8cI0eKy1u\n1xqEt0iHCFhBKoKLt1Q93t2mySPP8hAkvzYMVul0B3IjYjAPQjD/IN6PPzwtoiuk\nAL5s61vc3xuWyE/QzMtrU9NHDMcZ64XqPsq+nHpVTAHx1G8kSHtftLBeyAX8O0He\nTJqZ5R9ccxCjKsr5/0ltG8v6w9IM1QdvP3yYotyEFLTJzDZVkq2ZwuhuU7fcOOsa\n7IryzSrikQaUXuZnXk3lF8vgRSyTxdUTkB2dhgSpkQKBgQDPWt+dmZfTig/9mQMO\nabCy4UXCZ2IOXIv5ftYm6F4izZacpybrRN5Qd6KQE2X9UtSFAKcTREY+kpE88elS\nN42+9FgSw3j3iIA/DrLv+l3ALOjDSsPDFiJK16WDEqEwJjXHaVJGiKKxvNsez5co\nHz12tZdzVM6cuMU3ji8SeD3plQKBgQDH7z4BS/MOV45Y8j+JyREniI0NrbcqpjlD\nZbHB92oW5syN0it9cUr5e0fETbWuZtqpnvYf00fd3iPrNbhFyF96p9VTDyYSETxW\nTN9LKld+NBMFl/fKgTCACrdugFHUEY38aadEgqsrVfJnwJlkv1xGqT2iXPnuFAXZ\nMbSJrU3QlwKBgFuaMZ9V8xSosVe++PKtDjp3+nFfapodmFioB8M4dB275QVfcnoM\n+olh0JcFetDmfqwjFawVRVLvPOpHlGOV1zC8kQv4/r+btcwlmjcZy8DSFiHwLXtJ\nk+ysR5oX6M10k918UbSHOBh87fA2lPhMlMKUhkuU85GYrsdUMMdgBznhAoGAH6OR\neV0Oeo+KfDLhsSk7aLkoMSjzWnFxkGPQbHR9umANsMe4kCJUjgRfSAZQIOgv5DW+\nlyy1K7wSjQ80OP7LdTqH/JviF2vY04NYtqldrSAKfpEc6lPlW+3WN0thJ6Ebif0i\nNipDchkowhhtbc8GiDwR/9acynsqrcM64aqmLMMCgYEAoGqJSqQB6MCntAQKNFId\n53SLylNC+GVF/aQhcoLXu6BvbsMB4lOGml8CQYb74OSKEHFP82IHg6Km+C5OyR1K\n0FQ8rGpGb4KU7V+JdjsdRdhAAe9f+uX2+wbRDSKtaXFz1Mcn9ilKKb8G7R2q5pFW\n1qt3iuYPxoZ+tbrXQbUCc0A=\n-----END PRIVATE KEY-----\n",
  "client_email": "935008526857-compute@developer.gserviceaccount.com",
  "client_id": "108443447170787104385",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://accounts.google.com/o/oauth2/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/935008526857-compute%40developer.gserviceaccount.com"
}
                       """
  #
  # ID of the GCP region to use (must be available within the project)
  # See: https://cloud.google.com/compute/docs/zones
  region: us-central1
}

#
# SSH credentials to use to connect to the instances
#
ssh {
  username: gregj
  privateKey: /home/gregj/.ssh/google_compute_engine # with an absolute path to the SSH key you want to use
}

#
# A list of instance types to use for group of nodes or management services
#
instances {
  4core-no-data-cm {
    # No data disks, used for CM instance
    instanceNamePrefix: gg-cm
    # image: centos6
    image: "https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-7-v20161027"
    # image: "https://www.googleapis.com/compute/v1/projects/gcp-se/global/images/cm59-prebaked"
    networkName: locked-down
    # The type of machine to use for this instance (c.f. https://cloud.google.com/compute/docs/machine-types)
    type: n1-highmem-4
    # The zone ID  (c.f. https://cloud.google.com/compute/docs/zones)
    #
    zone: us-central1-c
    bootDiskType: Standard
    bootDiskSizeGb: 60
    dataDiskCount: 0
    tags {
      owner: gregoryg
    }
    bootstrapScript: """#!/bin/sh
yum -y install centos-release-scl wget
cd /tmp

wget 'http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.0.0.cloudera1.jar'
wget 'http://archive.cloudera.com/beta/kudu/csd/KUDU-1.1.0.jar'

mkdir -p /opt/cloudera/csd
mv -v SPARK2_ON_YARN-2.0.0.cloudera1.jar /opt/cloudera/csd/
mv -v KUDU-1.1.0.jar /opt/cloudera/csd/
exit 0
"""
  }
  4core-no-data-gateway {
    # No data disks, used for gateway instances
    instanceNamePrefix: gg-gw
    # image: centos6
    image: "https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-7-v20161027"
    # image: "https://www.googleapis.com/compute/v1/projects/gcp-se/global/images/cm59-prebaked"
    networkName: locked-down
    # The type of machine to use for this instance (c.f. https://cloud.google.com/compute/docs/machine-types)
    type: n1-highmem-4
    # The zone ID  (c.f. https://cloud.google.com/compute/docs/zones)
    #
    zone: us-central1-c
    bootDiskType: Standard
    bootDiskSizeGb: 60
    dataDiskCount: 0
    tags {
      owner: gregoryg
    }
    bootstrapScript: """#!/bin/sh
yum -y install centos-release-scl
exit 0
"""
  }
  4core-no-data-master {
    # No data disks, used for master instances
    instanceNamePrefix: gg-master
    # image: centos6
    image: "https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-7-v20161027"
    # image: "https://www.googleapis.com/compute/v1/projects/gcp-se/global/images/cm59-prebaked"
    networkName: locked-down
    # The type of machine to use for this instance (c.f. https://cloud.google.com/compute/docs/machine-types)
    type: n1-highmem-4
    zone: us-central1-c
    bootDiskType: Standard
    bootDiskSizeGb: 60
    dataDiskCount: 0
    tags {
      owner: gregoryg
    }
    bootstrapScript: """#!/bin/sh
yum -y install centos-release-scl
exit 0
"""
  }
  4core-2-1tb-data {
    # 2 data disks, used for worker instances
    instanceNamePrefix: gg-worker
    # image: centos6
    image: "https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-7-v20161027"
    # image: "https://www.googleapis.com/compute/v1/projects/gcp-se/global/images/cm59-prebaked"
    networkName: locked-down
    type: n1-highmem-4
    zone: us-central1-c
    bootDiskType: Standard
    bootDiskSizeGb: 60
    dataDiskCount: 2
    dataDiskType: Standard
    dataDiskSizeGb: 1000
    tags {
      owner: gregoryg
    }
    bootstrapScript: """#!/bin/sh
yum -y install centos-release-scl
exit 0
"""
  }
}

#
# Optional external database server configuration.
#
# Cloudera Director can create databases on existing database servers or
# provision RDS database servers on-demand.
#
databaseServers {
  mysqlmeta {
    type: mysql
    host: ${?HOSTNAME}
    port: 3306
    user: cmdbadmin
    password: cmdbadmin
  }
}

#
# Configuration for Cloudera Manager. Cloudera Director can use an existing Cloudera Manager
# or bootstrap everything from scratch for a new cluster
#
cloudera-manager {
  repository: "http://archive.cloudera.com/cm5/redhat/7/x86_64/cm/5.9/"
  repositoryKeyUrl: "http://archive.cloudera.com/cm5/redhat/7/x86_64/cm/RPM-GPG-KEY-cloudera"

  configs {
    CLOUDERA_MANAGER {
      enable_api_debug: true
      custom_banner_html: "I hate Momomonsdays"
      # "\"What is my purpose?\" \"You pass butter\""/n33dful-g00dz/lucius
      # "Keep Summer Safe" OR "Peace among worlds!"/teenyverse/zeep
      # "Don't even hesitate, don't even worry"/realfakedoors.com/sandwich
      # "I hate Momomonsdays"/Gazorpazorpfield
      # "Oh I love me Strawberry Smiggles!"/entrails/smiggles
      enable_faster_bootstrap: true
      enable_fast_dir_create: true
    }

  }
  instance: ${instances.4core-no-data-cm} {
    tags {
      application: "Cloudera Manager 5"
    }
  }
  enableEnterpriseTrial: true

  # Install unlimited strength JCE policy files?
  unlimitedJce: true

  # #
  # # Kerberos Credentials
  # #

  # # #
  # # # An administrative Kerberos account capable of creating principals on the KDC that
  # # # Cloudera Manager will be using. This will typically be in the format:
  # # #    Principal@YOUR.KDC.REALM
  # krbAdminUsername: "cloudera-scm@GREGORYG.COM"
  # #
  # # # The password for the administrative Kerberos account.
  # krbAdminPassword: "zlort0"
  # #


  databaseTemplates {
    CLOUDERA_MANAGER {
      name: cmtemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: scm
      usernamePrefix: cmadmin
    }

    ACTIVITYMONITOR {
      name: cmtemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: amon
      usernamePrefix: cmadmin
    }

    REPORTSMANAGER {
      name: cmtemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: rptman
      usernamePrefix: cmadmin
    }


    NAVIGATOR {
      name: cmtemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: nav
      usernamePrefix: cmadmin
    }

    # Added in Cloudera Manager 5.2+
    NAVIGATORMETASERVER {
      name: cmtemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: navmeta
      usernamePrefix: cmadmin
    }
  }
}

#
# Cluster description
#
cluster {
  # Optional override of CDH parcel repositories
  parcelRepositories: [
    "http://archive.cloudera.com/cdh5/parcels/5.9/",
    "http://archive.cloudera.com/kafka/parcels/latest/",
    "http://archive.cloudera.com/beta/kudu/parcels/latest/",
    "http://archive.cloudera.com/beta/impala-kudu/parcels/latest/",
    "http://archive.cloudera.com/beta/recordservice/parcels/latest/",
    "http://archive.cloudera.com/spark2/parcels/2.0/",
  ]
  products {
    CDH: 5.9.0
    # KAFKA: 2
    KUDU: 1.1.0
    SPARK2: 2
    # SPARK_ON_YARN: 1
  }

  services: [HDFS, YARN, ZOOKEEPER, HIVE, HUE, OOZIE, IMPALA, SOLR, FLUME, SQOOP, SPARK2_ON_YARN, KUDU]

  ## NOTE: to get all role types for CM, use the API:
  ##     curl -u 'admin:admin'  http://10.240.0.3:7180/api/v14/clusters/lemoncheeks/services  ## to get the name of the service
  ##     curl -u 'admin:admin'  http://10.240.0.3:7180/api/v14/clusters/lemoncheeks/services/CD-IMPALA-CWufpYcy/roleTypes


  # S3 Configurations
  configs {
    HDFS {
      core_site_safety_valve: """
            <property>
                <name>fs.s3a.access.key</name>
                <value>"""${AWS_ACCESS_KEY_ID}"""</value>
            </property>
            <property>
                <name>fs.s3a.secret.key</name>
                <value>"""${AWS_SECRET_ACCESS_KEY}"""</value>
            </property>
        """
    }
  }

  databaseTemplates: {
    HIVE {
      name: hivetemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: hivemeta
      usernamePrefix: hive
    },
    # SENTRY {
    #   name: sentrytemplate
    #   databaseServerName: mysqlmeta # Must correspond to an external database server named above
    #   databaseNamePrefix: sentry
    #   usernamePrefix: sentry
    # },
    HUE {
      name: huetemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: hue
      usernamePrefix: hue
    },
    OOZIE {
      name: oozietemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: oozie
      usernamePrefix: oozie
    },
    SQOOP {
      name: sqooptemplate
      databaseServerName: mysqlmeta # Must correspond to an external database server named above
      databaseNamePrefix: sqoop
      usernamePrefix: sqoop
    }
  }

  gateways {
    count: 1
    instance: ${instances.4core-no-data-gateway} {
      instanceNamePrefix: gg-gw
      tags {
        group: gateway
      }
    }
    roles {
      FLUME: [AGENT]
      # HBASE: [GATEWAY]
      HDFS: [GATEWAY]
      HIVE: [GATEWAY]
      # KAFKA: [GATEWAY]
      SOLR: [GATEWAY]
      # SPARK_ON_YARN: [GATEWAY]
      SPARK2_ON_YARN: [GATEWAY]
      YARN: [GATEWAY]
      # SENTRY: [GATEWAY]
    }
  }

  masters {
    count: 1
    instance: ${instances.4core-no-data-master} {
      type: n1-highmem-8
      instanceNamePrefix: gg-master
      tags {
        group: master
      }

    }
    roles {
      HDFS: [NAMENODE, SECONDARYNAMENODE, BALANCER]
      YARN: [RESOURCEMANAGER, JOBHISTORY]
      ZOOKEEPER: [SERVER]
      # HBASE: [MASTER, HBASETHRIFTSERVER]
      HIVE: [HIVESERVER2, HIVEMETASTORE, WEBHCAT]
      IMPALA: [STATESTORE, CATALOGSERVER]
      HUE: [HUE_SERVER]
      # SPARK_ON_YARN: [SPARK_YARN_HISTORY_SERVER, GATEWAY]
      SPARK2_ON_YARN: [SPARK2_YARN_HISTORY_SERVER, GATEWAY]
      # KAFKA: [KAFKA_BROKER]
      # KS_INDEXER: [HBASE_INDEXER]
      OOZIE: [OOZIE_SERVER]
      # SENTRY: [SENTRY_SERVER]
      SOLR: [SOLR_SERVER]
      SQOOP: [SQOOP_SERVER]
      KUDU: [KUDU_MASTER]
    }
    # Optional custom role configurations
    # Configuration keys containing special characters (e.g., '.', ':') must be enclosed in double quotes.
    #
    configs {
      HDFS {
        NAMENODE {
          namenode_java_heapsize: 4294967296
          # dfs_name_dir_list: /data/nn
          # namenode_port: 1234
        }
        SECONDARYNAMENODE {
          secondary_namenode_java_heapsize: 4294967296
        }
      },
      KUDU {
        KUDU_MASTER {
          fs_wal_dir: "/data0/kudu/masterwal"
          fs_data_dirs: "/data1/kudu/master"
        }
      }
      # KAFKA {
      #    KAFKA_BROKER {
      #      broker_max_heap_size: 512
      #      # "log.dirs": /data0/kafka/data
      #   }
      # }
    }
  }

  workers {
    count: 2
    minCount: 1
    instance: ${instances.4core-2-1tb-data} {
      type: n1-highmem-8
      dataDiskCount: 4
      dataDiskType: Standard
      dataDiskSizeGb: 500
      instanceNamePrefix: gg-work
      tags {
        group: worker
      }
    }
    roles {
      HDFS: [DATANODE]
      YARN: [NODEMANAGER]
      IMPALA: [IMPALAD]
      # SPARK_ON_YARN: [GATEWAY]
      SPARK2_ON_YARN: [GATEWAY]
      # HBASE: [REGIONSERVER]
      FLUME: [AGENT]
      KUDU: [KUDU_TSERVER]
    }
    configs {
      KUDU {
        KUDU_TSERVER {
          fs_wal_dir: "/data2/kudu/tabletwal"
          fs_data_dirs: "/data3/kudu/tablet"
        }
      }
    }
  }

  zoo-workers {
    count: 2
    minCount: 2
    instance: ${instances.4core-2-1tb-data} {
      type: n1-highmem-8
      dataDiskCount: 4
      dataDiskType: Standard
      dataDiskSizeGb: 500
      instanceNamePrefix: gg-zwork
      tags {group: worker}
    }
    roles {
      HDFS: [DATANODE]
      YARN: [NODEMANAGER]
      IMPALA: [IMPALAD]
      ZOOKEEPER: [SERVER]
      # SPARK_ON_YARN: [GATEWAY]
      SPARK2_ON_YARN: [GATEWAY]
      # HBASE: [REGIONSERVER]
      FLUME: [AGENT]
      KUDU: [KUDU_TSERVER]
    }
    configs {
      KUDU {
        KUDU_TSERVER {
          fs_wal_dir: "/data2/kudu/tabletwal"
          fs_data_dirs: "/data3/kudu/tablet"
        }
      }
    }
  }

  task-runner {
    count: 0
    instanceNamePrefix: gg-task
    instance: ${instances.4core-no-data-master} {
      tags {group: taskrunner}
    }
  }

  ## one can use a combination of postCreateScripts and postCreateScriptPaths ; scripts are run on one random host
  postCreateScripts: ["""#!/bin/bash
yum -y install epel-release
yum -y install python-pip wget curl jq htop mlocate finger
pip install cm-api
""",
"""#!/usr/bin/python
import os
import time
from cm_api.api_client import ApiResource
# If the exit code is not zero Cloudera Director will fail

# Post creation scripts also have access to the following environment variables:

#    DEPLOYMENT_HOST_PORT
#    ENVIRONMENT_NAME
#    DEPLOYMENT_NAME
#    CLUSTER_NAME
#    CM_USERNAME
#    CM_PASSWORD

# echo "TODO: Installing Anaconda python parcel"
# echo "Add repository"

def main():
    anaconda_repo="https://repo.continuum.io/pkgs/misc/parcels/"
    cmhost = os.environ['DEPLOYMENT_HOST_PORT'].split(":")[0]
    api = ApiResource(cmhost, username='admin', password='admin')
    all_clusters = api.get_all_clusters()
    for cluster in all_clusters:
        if (cluster.name == os.environ['CLUSTER_NAME']):
            break

    cm = api.get_cloudera_manager()
    config = cm.get_config(view='summary')
    parcel_urls = config.get("REMOTE_PARCEL_REPO_URLS","").split(",")
    print "Adding Anaconda parcel repository"
    if anaconda_repo in parcel_urls:
        # already there, skip
        print "Anaconda repo url already included"
    else:
        parcel_urls.append(anaconda_repo)
        config["REMOTE_PARCEL_REPO_URLS"] = ",".join(parcel_urls)
        cm.update_config(config)
        # wait to make sure parcels are refreshed
        time.sleep(10)
        print "Added Anaconda repository URL"

    # Download Anaconda parcel
    print "Downloading parcel"
    parcel = cluster.get_parcel('Anaconda', '4.0.0')
    parcel.start_download()
    while True:
        parcel = cluster.get_parcel('Anaconda', '4.0.0')
        if parcel.stage == 'DOWNLOADED':
            break
        if parcel.state.errors:
            raise Exception(str(parcel.state.errors))
        print "   progress: %s of %s" % (parcel.state.progress, parcel.state.totalProgress)
        time.sleep(2)
    print "Downloaded"

    print "Distributing"
    parcel.start_distribution()
    while True:
        parcel = cluster.get_parcel('Anaconda', '4.0.0')
        if parcel.stage == 'DISTRIBUTED':
            break
        if parcel.state.errors:
            raise Exception(str(parcel.state.errors))
        print "   progress: %s of %s" % (parcel.state.progress, parcel.state.totalProgress)
        time.sleep(5)
    print "Distributed"

    print "Activating Anaconda"
    parcel.activate()
    while True:
        parcel = cluster.get_parcel('Anaconda', '4.0.0')
        if parcel.stage == 'ACTIVATED':
            break
        time.sleep(2)
    print "Activated. Anaconda is ready to use"

    
if __name__ == "__main__":
    main()

    """]

  # For more complex scripts, post creation scripts can be supplied via path,
  # where they will be read from the local filesystem.  They will run after
  # any scripts supplied in the previous postCreateScripts section.
  # postCreateScriptsPaths: ["/tmp/test-script.sh",
  #                         "/tmp/test-script.py"]

  preTerminateScripts: ["""#!/bin/sh

# This is an embedded pre-termination script that runs as root and can be used to
# customize the cluster after it has been created.

# If the exit code is not zero Cloudera Director will fail

# Pre terminate scripts also have access to the following environment variables:

#    DEPLOYMENT_HOST_PORT
#    ENVIRONMENT_NAME
#    DEPLOYMENT_NAME
#    CLUSTER_NAME
#    CM_USERNAME
#    CM_PASSWORD

echo 'Goodbye World!'
exit 0
    """,
    """#!/usr/bin/python

# Additionally, multiple pre terminate scripts can be supplied.  They will run
# in the order they are listed here.  Interpeters other than bash can be used
# as well.

print 'Goodbye again!'
        """]

  # For more complex scripts, pre terminate scripts can be supplied via path,
  # where they will be read from the local filesystem.  They will run after
  # any scripts supplied in the previous preTerminateScripts section.
  # preTerminateScriptsPaths: ["/tmp/test-script.sh",
  #                            "/tmp/test-script.py"]
}
