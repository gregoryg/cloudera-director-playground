#
# Copyright (c) 2015 Cloudera, Inc. All rights reserved.
#

#
# Guru Medasani AWS Cloudera Director configuration file based on the Cloudera AWS reference architecture:
# http://www.cloudera.com/content/cloudera/en/resources/library/whitepaper/cloudera-enterprise-reference-architecture-for-aws-deployments.html
#

#
# Cluster name
#
name: gmedasani-director-demo-1

### Note: Always delete the EBS Volumnes manually after removing the CDH cluster.

#
# Cloud provider configuration (credentials, region or zone and optional default image)
#
provider {
    type: aws
    accessKeyId: <AWS-ACCESS-KEY>
    secretAccessKey: <AWS-SECRET-ACCESS-KEY>
    region: us-east-1
    subnetId: subnet-e4368d92

    # Comma separated list of security group IDs
    securityGroupsIds: sg-32d72848

    # A prefix that Cloudera Director should use when naming the instances (this is not part of the hostname)
    instanceNamePrefix: gm-director

    rootVolumeSizeGB: 200 # defaults to 50 GB if not specified
    rootVolumeType: gp2 # OR standard (for EBS magnetic)
    associatePublicIpAddresses: true
}

# SSH credentials to use to connect to the instances
ssh {
    username: ec2-user # for RHEL image
    privateKey: /tmp/gmedasani.pem # with an absolute path to .pem file
}

#
# A list of instance types to use for group of nodes or management services
#
instances {

  m42x {
    type: m4.2xlarge  
    image: ami-2051294a    
    tags {
      owner: "gmedasani"
      customer: "chicago"
    }

    #
    # Flag indicating whether to normalize the instance. Not setting normalization here implies that your
    # bootstrap script will take care of normalization. This is an advanced configuration that will require
    # assistance from Cloudera support.
    #
    # Normalization includes:
    #   downloading and installing packages
    #   minimizing swappiness
    #   increasing the maximun number of open files
    #   mounting ephemeral disks
    #   resizing the root partition.
    #
    # Defaults to true
    #
    normalizeInstance: true

    bootstrapScript: """#!/bin/sh
echo 'Hello World!'
## Install the required packages
yum install -y --nogpgcheck wget bind-utils vim 
curl -L http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-8.noarch.rpm -o /tmp/epel-release-7-8.noarch.rpm
yum install -y --nogpgcheck /tmp/epel-release-7-8.noarch.rpm
yum install -y --nogpgcheck python-pip
pip install awscli

## Install the mysql and postgres client libraries for CM, Hive, Oozie and Hue
curl -L http://archive.cloudera.com/director/redhat/7/x86_64/director/2.1.0/RPMS/x86_64/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm -o 
/tmp/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm

yum install -y --nogpgcheck /tmp/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm
yum install -y --nogpgcheck mysql-connector-java
curl -L https://jdbc.postgresql.org/download/postgresql-9.4.1209.jar -o /usr/share/java/postgresql-9.4.1209.jdbc4.jar
exit 0"""

  }

  c38x {
    type: c3.8xlarge
    image: ami-2051294a
    normalizeInstance: true
    tags {
      owner: "gmedasani"
      customer: "chicago"
    }
    

   bootstrapScript: """#!/bin/sh
echo 'Hello World!'
## Install the required packages
yum install -y --nogpgcheck wget bind-utils vim 
curl -L http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-8.noarch.rpm -o /tmp/epel-release-7-8.noarch.rpm
yum install -y --nogpgcheck /tmp/epel-release-7-8.noarch.rpm
yum install -y --nogpgcheck python-pip
curl -L http://archive.cloudera.com/director/redhat/7/x86_64/director/2.1.0/RPMS/x86_64/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm -o /tmp/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm
yum install -y --nogpgcheck /tmp/oracle-j2sdk1.8-1.8.0+update60-1.x86_64.rpm
sleep 10
pip install awscli

## Install the mysql and postgres client libraries for CM, Hive, Oozie and Hue
yum install -y --nogpgcheck mysql-connector-java
curl -L https://jdbc.postgresql.org/download/postgresql-9.4.1209.jar -o /usr/share/java/postgresql-9.4.1209.jdbc4.jar
exit 0"""
  }


  c381x {
    type: c3.8xlarge
    image: ami-2051294a
    normalizeInstance: true
    tags {
      owner: "gmedasani"
      customer: "chicago"
    }
    

   bootstrapScript: """#!/bin/sh
echo 'Hello World!'
## Install the required packages
yum install -y --nogpgcheck wget bind-utils vim 
curl -L http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-8.noarch.rpm -o /tmp/epel-release-7-8.noarch.rpm
yum install -y --nogpgcheck /tmp/epel-release-7-8.noarch.rpm
yum install -y --nogpgcheck python-pip
pip install awscli

## Set AWS configuration
export AWS_ACCESS_KEY_ID=<AWS-ACCESS-KEY>
export AWS_SECRET_ACCESS_KEY=<AWS-SECRET-ACCESS-KEY>
export INSTANCE_ID=$(curl http://instance-data/latest/meta-data/instance-id)
export AVAILABILITY_ZONE=$(curl http://instance-data/latest/meta-data/placement/availability-zone)

## Create new Volumes
export VOL1_ID=$(aws ec2 create-volume --size 1024 --region us-east-1 --availability-zone ${AVAILABILITY_ZONE} --volume-type gp2 --query "VolumeId" | tr -d '"')
sleep 10

## Attach the volumes as block devices to the instance
aws ec2 attach-volume --region us-east-1 --volume-id ${VOL1_ID} --instance-id ${INSTANCE_ID} --device /dev/sdb

## Create filesystems on the devices
mkfs.xfs /dev/xvdb

## Create the data directories for hdfs
mkdir /data1

## Add the mount points to the /etc/fstab
echo "/dev/xvdb /data1    xfs    defaults,noatime     0 0" >> /etc/fstab
sleep 2

## Mount all the devices
mount -a
exit 0"""
  }

  m44x {
    type: m4.4xlarge
    image: ami-2051294a
    normalizeInstance: true
    tags {
      owner: "gmedasani"
      customer: "chicago"
    }
    
    
    bootstrapScript: """#!/bin/sh
echo 'Hello World!'
## Install the required packages
yum install -y --nogpgcheck wget bind-utils vim 
curl -L http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-8.noarch.rpm -o /tmp/epel-release-7-8.noarch.rpm
yum install -y --nogpgcheck /tmp/epel-release-7-8.noarch.rpm
yum install -y --nogpgcheck python-pip
pip install awscli

## Set AWS configuration
export AWS_ACCESS_KEY_ID=<AWS-ACCESS-KEY>
export AWS_SECRET_ACCESS_KEY=<AWS_SECRET_ACCESS_KEY>
export INSTANCE_ID=$(curl http://instance-data/latest/meta-data/instance-id)
export AVAILABILITY_ZONE=$(curl http://instance-data/latest/meta-data/placement/availability-zone)

## Create new Volumes
export VOL1_ID=$(aws ec2 create-volume --size 1024 --region us-east-1 --availability-zone ${AVAILABILITY_ZONE} --volume-type gp2 --query "VolumeId" | tr -d '"')
export VOL2_ID=$(aws ec2 create-volume --size 1024 --region us-east-1 --availability-zone ${AVAILABILITY_ZONE} --volume-type gp2 --query "VolumeId" | tr -d '"')
export VOL3_ID=$(aws ec2 create-volume --size 1024 --region us-east-1 --availability-zone ${AVAILABILITY_ZONE} --volume-type gp2 --query "VolumeId" | tr -d '"')
export VOL4_ID=$(aws ec2 create-volume --size 1024 --region us-east-1 --availability-zone ${AVAILABILITY_ZONE} --volume-type gp2 --query "VolumeId" | tr -d '"')
sleep 30

## Attach the volumes as block devices to the instance
aws ec2 attach-volume --region us-east-1 --volume-id ${VOL1_ID} --instance-id ${INSTANCE_ID} --device /dev/sdb
aws ec2 attach-volume --region us-east-1 --volume-id ${VOL2_ID} --instance-id ${INSTANCE_ID} --device /dev/sdc
aws ec2 attach-volume --region us-east-1 --volume-id ${VOL3_ID} --instance-id ${INSTANCE_ID} --device /dev/sdd
aws ec2 attach-volume --region us-east-1 --volume-id ${VOL4_ID} --instance-id ${INSTANCE_ID} --device /dev/sdf

## Create filesystems on the devices
mkfs.xfs /dev/xvdb
mkfs.xfs /dev/xvdc
mkfs.xfs /dev/xvdd
mkfs.xfs /dev/xvdf

## Create the data directories for hdfs
mkdir /data1
mkdir /data2
mkdir /data3
mkdir /data4

## Add the mount points to the /etc/fstab
echo "/dev/xvdb /data1    xfs    defaults,noatime     0 0" >> /etc/fstab
echo "/dev/xvdc /data2    xfs    defaults,noatime     0 0" >> /etc/fstab
echo "/dev/xvdd /data3    xfs    defaults,noatime     0 0" >> /etc/fstab
echo "/dev/xvdf /data4    xfs    defaults,noatime     0 0" >> /etc/fstab
sleep 10

## Mount all the devices
mount -a

exit 0"""
  }
}

#
# Optional external database server configuration.
#
# Cloudera Director can create databases on existing database servers or
# provision RDS database servers on-demand.
#
 databaseServers {
    
    gmDirectorDemoRDS {
      type: mysql
      user: username
      password: password
      instanceClass: db.m3.2xlarge
      dbSubnetGroupName: default
      vpcSecurityGroupIds: sg-32d72848
      allocatedStorage: 200
      engineVersion: 5.6.27
      tags {
        owner: gmedasani
        customer: chicago
      }
    }
}

#
# Configuration for Cloudera Manager. Cloudera Director can use an existing Cloudera Manager
# or bootstrap everything from scratch for a new cluster
#

cloudera-manager {

     instance: ${instances.m42x} {
        placementGroup: guru-placement-group
        tags {
            # add any additional tags as needed
            application: "Director Cloudera Manager Demo"
        }
     }

    #
    # Licensing configuration
    #
    # There are three mutually exclusive options for setting up Cloudera Manager's license.
    # 1. License text may be embedded in this file using the "license" field. Triple quotes (""")
    #    are recommended for including multi-line text strings.
    # 2. The "licensePath" can be used to specify the path to a file containing the license.
    # 3. The "enableEnterpriseTrial" flag indicates whether the 60-Day Cloudera Enterprise Trial
    #    should be activated when no license is present. This must not be set to true if a
    #    license is included using either "license" or "licensePath".

    #
    # Embed a license for Cloudera Manager
    #

    license: """
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

{
  "version"        : 1,
  "name"           : "Philip Langdale",
  "uuid"           : "12c8052f-d78f-4a8e-bba4-a55a2d141fcc",
  "expirationDate" : null,
  "features"       : [ "IMPALA" ]
}
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.11 (GNU/Linux)

iQIcBAEBAgAGBQJQa3D6AAoJEFrC2ATt5goQ5YQP/RWBLYoSornwmICjm9c/G2/J
yPPExxcvosLUuOuc2czzXH0mWbibEO6tO7Axzw+th9snzgKSVJy2A8Y7auSPGfh9
NPNeTL1L4/wpAyr9nA/s6f38IyUs79CnQQPaLs3BC76mwntcjqdpCmMgb++7LJKW
p3DUoCV6QNiWbSekiX9m/QnY+OyH5b/QuPekCpFucSuU73k9zycEcUmjOUdDWuEn
3WN3HUwEk9cumDFP5ftQX+EYNUwoQS3lHydYc2juY9pS883IjU7mJKlil1L1XeIN
5vYJ/Jm1coo9fiesZygaMASz9O5pB8PV+Hbb7mvyyK8enKrjZeQZNJ+9FPjRGhp7
E5+aj/a1QrYzjyIjQJJutxFZ1WGrT+NLykH/StxJRx/jeRj8U1U07DKuqnXjc0vR
JOXkatc+sewDrTDPNXoiwNzRRCrZkHar2wS3kaLasdtQhE/Saov/Fy7dmwaugtLv
qKYuFkUYVS+UvuB6L3NGverR8fDYwvCpf/aWCnt98Hdne25zBmjTzqxdkP0IhLGz
V25jJb9mapZT3gG9qm+cogynAZCwxPem00MU/X0r4IaZQwuwu3yPpfox60acxQEr
5VslGlbFhud65h04ONJ30Ob/lGF0Adm9VkzB5KrmCvFm4dYde7jQdeS3UFID8wVh
MOlCM6fLmQcV9GPac2Yq
=0JMP
-----END PGP SIGNATURE-----"""


    billingId: 0018000000rBcoZAAS

    #
    # Optional database configuration
    #
    # There are three mutually exclusive options for database usage in Cloudera Director.
    # 1. With no configuration, an embedded PostgreSQL database will be used.
    # 2. Alternatively, existing external databases can be used.
    # 3. Finally, databases can be created on the fly on existing external database servers.

    #
    # Optional configuration for creating external databases on the fly
    #
    # When a database is created on the fly, Director generates a random database name using the specified database
    # name prefix, a random username based on the specified username prefix, and a random password. The password is
    # stored by Director and made available to the service that uses the database. If multiple services reference the
    # same external database server, Director will create a database for each.
    #
    # MySQL limits usernames to sixteen characters. Therefore, limit usernamePrefix values for databases on MySQL to
    # seven characters; the remaining nine characters are used by the randomized suffix generated by Director.
    #

     databaseTemplates {
        CLOUDERA_MANAGER {
             name: cmtemplate
             databaseServerName: gmDirectorDemoRDS # Must correspond to an external database server named above
             databaseNamePrefix: scmdb
             usernamePrefix: scm
         }
    
         ACTIVITYMONITOR {
             name: amontemplate
             databaseServerName: gmDirectorDemoRDS # Must correspond to an external database server named above
             databaseNamePrefix: amondb
             usernamePrefix: amon
         }
    
         REPORTSMANAGER {
             name: rmantemplate
             databaseServerName: gmDirectorDemoRDS # Must correspond to an external database server named above
             databaseNamePrefix: rmandb
             usernamePrefix: rman
         }
    
         NAVIGATOR {
             name: navtemplate
             databaseServerName: gmDirectorDemoRDS # Must correspond to an external database server named above
             databaseNamePrefix: navdb
             usernamePrefix: nav
         }
    
         # Added in Cloudera Manager 5.2+
         NAVIGATORMETASERVER {
             name: navmetatemplate
             databaseServerName: gmDirectorDemoRDS # Must correspond to an external database server named above
             databaseNamePrefix: navmtdb
             usernamePrefix: navmt
         }
    }

    #
    # Configuration to override Cloudera Manager package repositories
    #

    repository: "http://archive.cloudera.com/cm5/redhat/7/x86_64/cm/5.8.1/"
    repositoryKeyUrl: "http://archive.cloudera.com/cm5/redhat/7/x86_64/cm/RPM-GPG-KEY-cloudera"

     configs {
         # CLOUDERA_MANAGER corresponds to the Cloudera Manager Server configuration options
         CLOUDERA_MANAGER {
             enable_api_debug: true
             custom_banner_html: "Wow!!!! This Cluster is Managed by Cloudera Director ;-) "
         }
    
         # CLOUDERA_MANAGEMENT_SERVICE corresponds to the Service-Wide configuration options
         CLOUDERA_MANAGEMENT_SERVICE {
             enable_alerts : false
             enable_config_alerts : false
         }

             #     SERVICEMONITOR { ... }
    #
    #     ACTIVITYMONITOR { ... }
    #
    #     HOSTMONITOR { ... }
    #
    #     REPORTSMANAGER { ... }
    #
    #     EVENTSERVER { ... }
    #
    #     ALERTPUBLISHER { ... }
    #
    #     NAVIGATOR { ... }
    #
    #     # Added in Cloudera Manager 5.2+
    #     NAVIGATORMETASERVER { ... }
    #
    #     # Configuration properties for all hosts
    #     HOSTS { ... }

    }
}

#
# Cluster description
#

cluster {

    products {
      CDH: 5.8.0
      #KAFKA: 2.0.2
    }

    parcelRepositories: ["http://archive.cloudera.com/cdh5/parcels/5.8.0/"]

    # NOTE: Sentry is only supported in Cloudera Manager versions 5.1 onward.
    #services: [HDFS, YARN, ZOOKEEPER, HBASE, HIVE, HUE, OOZIE, SENTRY, SPARK_ON_YARN, KAFKA]
    services: [HDFS, YARN, HIVE, HUE, OOZIE, ZOOKEEPER]

    #
    # Optional custom service configurations
    # Configuration keys containing special characters (e.g., '.', ':') must be enclosed in double quotes.
    #
    # configs {
    #     HDFS {
    #       dfs_block_size: 134217728
    #     }
    #
    #     MAPREDUCE {
    #       mapred_system_dir: /user/home
    #       mr_user_to_impersonate: mapred1
    #     }
    #
    #     KAFKA {
    #       "num.partitions": 3
    #     }
    # }

    #
    # Optional configuration for existing external database for Hive Metastore or Sentry databases
    #
    # databases {
    #     HIVE {

    #     }

    #     HUE {

    #     }
         
    #     OOZIE {

    #     }
    #     SENTRY {

    #     }
    #}

    #
    # Optional configuration for creating external database on the fly for Hive Metastore or Sentry
    # database
    #

     databaseTemplates: {
         HIVE {
             name: hivetemplate
             databaseServerName: gmDirectorDemoRDS # Must correspond to an external database server named above
             databaseNamePrefix: hivedb
             usernamePrefix: hive
         }
         HUE {
             name: huetemplate
             databaseServerName: gmDirectorDemoRDS # Must correspond to an external database server named above
             databaseNamePrefix: huedb
             usernamePrefix: hue
         }
         OOZIE {
             name: oozietemplate
             databaseServerName: gmDirectorDemoRDS # Must correspond to an external database server named above
             databaseNamePrefix: ooziedb
             usernamePrefix: oozie
         }

     }

    masters {
      count: 1
      minCount: 1
      instance: ${instances.c38x} {
        placementGroup: guru-placement-group
        tags {
          group: master
        }
      }

      roles {
        HDFS: [NAMENODE, SECONDARYNAMENODE]
        YARN: [RESOURCEMANAGER, JOBHISTORY]
        ZOOKEEPER: [SERVER]
        #HBASE: [MASTER]
        HIVE: [HIVESERVER2, HIVEMETASTORE]
        HUE: [HUE_SERVER]
        OOZIE: [OOZIE_SERVER]
        #SENTRY: [SENTRY_SERVER]
        #SPARK_ON_YARN: [SPARK_YARN_HISTORY_SERVER]
        #KAFKA: [KAFKA_BROKER]
      }

      # Optional custom role configurations
      # Configuration keys containing special characters (e.g., '.', ':') must be enclosed in double quotes.
      #
      configs {
        HDFS {
           NAMENODE {
             dfs_name_dir_list: "/data1/dfs/nn,/data2/dfs/nn"
           }
        }
      }
    }

    workers {
      count: 3
      minCount: 3
      instance: ${instances.m44x} {
        placementGroup: guru-placement-group
        tags {
          group: worker
        }
      }

      roles {
        HDFS: [DATANODE]
        YARN: [NODEMANAGER]
        #HBASE: [REGIONSERVER]
      }

      # Optional custom role configurations
      # Configuration keys containing periods must be enclosed in double quotes.
      configs {
        HDFS {
          DATANODE {
             dfs_data_dir_list: "/data1/dfs/dn,/data2/dfs/dn,/data3/dfs/dn,/data4/dfs/dn"
           }
        }
        YARN {
          NODEMANAGER {
            yarn_nodemanager_local_dirs: "/data1/yarn/nm,/data2/yarn/nm,/data3/yarn/nm,/data4/yarn/nm"
            yarn_nodemanager_log_dirs: "/data1/yarn/container-logs,/data2/yarn/container-logs,/data3/yarn/container-logs,/data4/yarn/container-logs"
          }
        }
      }
    }

    gateways {
      count: 1
      minCount: 1
      instance: ${instances.c381x} {
        placementGroup: guru-placement-group
        tags {
          group: gateway
        }
      }

      roles {
        HIVE: [GATEWAY]
        HDFS: [GATEWAY]
        YARN: [GATEWAY]
        #SPARK_ON_YARN: [GATEWAY]
      }
    }

    postCreateScripts: ["""#!/bin/sh

# This is an embedded post creation script that runs as root and can be used to
# customize the cluster after it has been created.

# If the exit code is not zero Cloudera Director will fail

# Post creation scripts also have access to the following environment variables:

#    DEPLOYMENT_HOST_PORT
#    ENVIRONMENT_NAME
#    DEPLOYMENT_NAME
#    CLUSTER_NAME
#    CM_USERNAME
#    CM_PASSWORD

echo 'Hello World!'
exit 0
    """,
    """#!/usr/bin/python

# Additionally, multiple post-creation scripts can be supplied.  They will run
# in the order they are listed here.  Interpeters other than bash can be used
# as well.

print 'Hello again!'
    """]

    # For more complex scripts, post creation scripts can be supplied via path,
    # where they will be read from the local filesystem.  They will run after
    # any scripts supplied in the previous postCreateScripts section.
    # postCreateScriptsPaths: ["/tmp/test-script.sh",
    #                         "/tmp/test-script.py"]

    preTerminateScripts: ["""#!/bin/sh

# This is an embedded pre-termination script that runs as root and can be used to
# customize the cluster after it has been created.

# If the exit code is not zero Cloudera Director will fail

# Pre terminate scripts also have access to the following environment variables:

#    DEPLOYMENT_HOST_PORT
#    ENVIRONMENT_NAME
#    DEPLOYMENT_NAME
#    CLUSTER_NAME
#    CM_USERNAME
#    CM_PASSWORD

echo 'Goodbye World!'
exit 0
    """,
    """#!/usr/bin/python

# Additionally, multiple pre terminate scripts can be supplied.  They will run
# in the order they are listed here.  Interpeters other than bash can be used
# as well.

print 'Goodbye again!'
        """]

    # For more complex scripts, pre terminate scripts can be supplied via path,
    # where they will be read from the local filesystem.  They will run after
    # any scripts supplied in the previous preTerminateScripts section.
    # preTerminateScriptsPaths: ["/tmp/test-script.sh",
    #                            "/tmp/test-script.py"]
}